* 
* ==> Audit <==
* |--------------|---------------------|----------|--------|---------|---------------------|---------------------|
|   Command    |        Args         | Profile  |  User  | Version |     Start Time      |      End Time       |
|--------------|---------------------|----------|--------|---------|---------------------|---------------------|
| start        |                     | minikube | apiiro | v1.31.2 | 13 Sep 23 17:08 IDT | 13 Sep 23 17:11 IDT |
| dashboard    |                     | minikube | apiiro | v1.31.2 | 13 Sep 23 17:13 IDT |                     |
| service      | hello-minikube      | minikube | apiiro | v1.31.2 | 13 Sep 23 17:16 IDT | 13 Sep 23 17:33 IDT |
| addons       | list                | minikube | apiiro | v1.31.2 | 13 Sep 23 17:33 IDT | 13 Sep 23 17:33 IDT |
| start        |                     | minikube | apiiro | v1.31.2 | 13 Sep 23 17:40 IDT | 13 Sep 23 17:40 IDT |
| update-check |                     | minikube | apiiro | v1.31.2 | 13 Sep 23 17:46 IDT | 13 Sep 23 17:46 IDT |
| start        |                     | minikube | apiiro | v1.31.2 | 17 Sep 23 21:20 IDT | 17 Sep 23 21:21 IDT |
| service      | myserverapp-service | minikube | apiiro | v1.31.2 | 17 Sep 23 21:26 IDT |                     |
| tunnel       |                     | minikube | apiiro | v1.31.2 | 17 Sep 23 21:35 IDT | 17 Sep 23 21:36 IDT |
| tunnel       |                     | minikube | apiiro | v1.31.2 | 17 Sep 23 21:36 IDT | 17 Sep 23 21:37 IDT |
| ip           |                     | minikube | apiiro | v1.31.2 | 17 Sep 23 21:37 IDT | 17 Sep 23 21:37 IDT |
|--------------|---------------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/09/17 21:20:36
Running on machine: apiiros-MacBook-Pro
Binary: Built with gc go1.21.0 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0917 21:20:36.484453   57881 out.go:296] Setting OutFile to fd 1 ...
I0917 21:20:36.485160   57881 out.go:348] isatty.IsTerminal(1) = true
I0917 21:20:36.485165   57881 out.go:309] Setting ErrFile to fd 2...
I0917 21:20:36.485172   57881 out.go:348] isatty.IsTerminal(2) = true
I0917 21:20:36.486520   57881 root.go:338] Updating PATH: /Users/apiiro/.minikube/bin
W0917 21:20:36.486852   57881 root.go:314] Error reading config file at /Users/apiiro/.minikube/config/config.json: open /Users/apiiro/.minikube/config/config.json: no such file or directory
I0917 21:20:36.488937   57881 out.go:303] Setting JSON to false
I0917 21:20:36.513749   57881 start.go:128] hostinfo: {"hostname":"apiiros-MacBook-Pro.local","uptime":627751,"bootTime":1694347085,"procs":597,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.6.6","kernelVersion":"21.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"b8a5edd5-cd7f-5aef-9b4d-1a237594b9f6"}
W0917 21:20:36.513877   57881 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0917 21:20:36.532742   57881 out.go:177] üòÑ  minikube v1.31.2 on Darwin 12.6.6
I0917 21:20:36.572933   57881 notify.go:220] Checking for updates...
I0917 21:20:36.573834   57881 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0917 21:20:36.574614   57881 driver.go:373] Setting default libvirt URI to qemu:///system
I0917 21:20:36.645726   57881 docker.go:121] docker version: linux-24.0.5:Docker Desktop 4.22.1 (118664)
I0917 21:20:36.646172   57881 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 21:20:36.932697   57881 info.go:266] docker info: {ID:455f27eb-4c38-4248-875f-d252e6374884 Containers:4 ContainersRunning:2 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:89 OomKillDisable:false NGoroutines:146 SystemTime:2023-09-17 18:20:36.902773205 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:31 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8240328704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/apiiro/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:/Users/apiiro/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:/Users/apiiro/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/apiiro/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/apiiro/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/Users/apiiro/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/apiiro/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/apiiro/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0917 21:20:36.952477   57881 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0917 21:20:36.973007   57881 start.go:298] selected driver: docker
I0917 21:20:36.973048   57881 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0917 21:20:36.973210   57881 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0917 21:20:36.973725   57881 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 21:20:37.090968   57881 info.go:266] docker info: {ID:455f27eb-4c38-4248-875f-d252e6374884 Containers:4 ContainersRunning:2 ContainersPaused:0 ContainersStopped:2 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:89 OomKillDisable:false NGoroutines:146 SystemTime:2023-09-17 18:20:37.075079865 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:31 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8240328704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/apiiro/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:/Users/apiiro/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:/Users/apiiro/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/apiiro/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/apiiro/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:/Users/apiiro/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/apiiro/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/apiiro/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0917 21:20:37.091632   57881 cni.go:84] Creating CNI manager for ""
I0917 21:20:37.091652   57881 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 21:20:37.091666   57881 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0917 21:20:37.130298   57881 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0917 21:20:37.149436   57881 cache.go:122] Beginning downloading kic base image for docker with docker
I0917 21:20:37.169207   57881 out.go:177] üöú  Pulling base image ...
I0917 21:20:37.207335   57881 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0917 21:20:37.207381   57881 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0917 21:20:37.207402   57881 preload.go:148] Found local preload: /Users/apiiro/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I0917 21:20:37.207411   57881 cache.go:57] Caching tarball of preloaded images
I0917 21:20:37.208326   57881 preload.go:174] Found /Users/apiiro/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0917 21:20:37.208385   57881 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0917 21:20:37.208685   57881 profile.go:148] Saving config to /Users/apiiro/.minikube/profiles/minikube/config.json ...
I0917 21:20:37.268276   57881 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0917 21:20:37.268289   57881 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0917 21:20:37.268312   57881 cache.go:195] Successfully downloaded all kic artifacts
I0917 21:20:37.268362   57881 start.go:365] acquiring machines lock for minikube: {Name:mk72b0b94d06d242726d4cbd7ed3dab8099d2b54 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0917 21:20:37.268579   57881 start.go:369] acquired machines lock for "minikube" in 197.414¬µs
I0917 21:20:37.268598   57881 start.go:96] Skipping create...Using existing machine configuration
I0917 21:20:37.268607   57881 fix.go:54] fixHost starting: 
I0917 21:20:37.268850   57881 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 21:20:37.325696   57881 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0917 21:20:37.325739   57881 fix.go:128] unexpected machine state, will restart: <nil>
I0917 21:20:37.363848   57881 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0917 21:20:37.383381   57881 cli_runner.go:164] Run: docker start minikube
I0917 21:20:38.220114   57881 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 21:20:38.279930   57881 kic.go:426] container "minikube" state is running.
I0917 21:20:38.281236   57881 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 21:20:38.339469   57881 profile.go:148] Saving config to /Users/apiiro/.minikube/profiles/minikube/config.json ...
I0917 21:20:38.339843   57881 machine.go:88] provisioning docker machine ...
I0917 21:20:38.339868   57881 ubuntu.go:169] provisioning hostname "minikube"
I0917 21:20:38.339930   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:38.396791   57881 main.go:141] libmachine: Using SSH client type: native
I0917 21:20:38.397430   57881 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 64444 <nil> <nil>}
I0917 21:20:38.397441   57881 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0917 21:20:38.398572   57881 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0917 21:20:41.560842   57881 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 21:20:41.560912   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:41.618676   57881 main.go:141] libmachine: Using SSH client type: native
I0917 21:20:41.619038   57881 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 64444 <nil> <nil>}
I0917 21:20:41.619048   57881 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0917 21:20:41.755474   57881 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0917 21:20:41.755495   57881 ubuntu.go:175] set auth options {CertDir:/Users/apiiro/.minikube CaCertPath:/Users/apiiro/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/apiiro/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/apiiro/.minikube/machines/server.pem ServerKeyPath:/Users/apiiro/.minikube/machines/server-key.pem ClientKeyPath:/Users/apiiro/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/apiiro/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/apiiro/.minikube}
I0917 21:20:41.755534   57881 ubuntu.go:177] setting up certificates
I0917 21:20:41.755544   57881 provision.go:83] configureAuth start
I0917 21:20:41.755622   57881 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 21:20:41.813540   57881 provision.go:138] copyHostCerts
I0917 21:20:41.813639   57881 exec_runner.go:144] found /Users/apiiro/.minikube/ca.pem, removing ...
I0917 21:20:41.813650   57881 exec_runner.go:203] rm: /Users/apiiro/.minikube/ca.pem
I0917 21:20:41.813772   57881 exec_runner.go:151] cp: /Users/apiiro/.minikube/certs/ca.pem --> /Users/apiiro/.minikube/ca.pem (1078 bytes)
I0917 21:20:41.814694   57881 exec_runner.go:144] found /Users/apiiro/.minikube/cert.pem, removing ...
I0917 21:20:41.814698   57881 exec_runner.go:203] rm: /Users/apiiro/.minikube/cert.pem
I0917 21:20:41.814774   57881 exec_runner.go:151] cp: /Users/apiiro/.minikube/certs/cert.pem --> /Users/apiiro/.minikube/cert.pem (1123 bytes)
I0917 21:20:41.815177   57881 exec_runner.go:144] found /Users/apiiro/.minikube/key.pem, removing ...
I0917 21:20:41.815181   57881 exec_runner.go:203] rm: /Users/apiiro/.minikube/key.pem
I0917 21:20:41.815256   57881 exec_runner.go:151] cp: /Users/apiiro/.minikube/certs/key.pem --> /Users/apiiro/.minikube/key.pem (1675 bytes)
I0917 21:20:41.815542   57881 provision.go:112] generating server cert: /Users/apiiro/.minikube/machines/server.pem ca-key=/Users/apiiro/.minikube/certs/ca.pem private-key=/Users/apiiro/.minikube/certs/ca-key.pem org=apiiro.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0917 21:20:41.942681   57881 provision.go:172] copyRemoteCerts
I0917 21:20:41.949741   57881 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0917 21:20:41.950009   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:42.007618   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:20:42.105042   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0917 21:20:42.133151   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0917 21:20:42.156183   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0917 21:20:42.179232   57881 provision.go:86] duration metric: configureAuth took 423.626685ms
I0917 21:20:42.179244   57881 ubuntu.go:193] setting minikube options for container-runtime
I0917 21:20:42.179421   57881 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0917 21:20:42.179468   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:42.235053   57881 main.go:141] libmachine: Using SSH client type: native
I0917 21:20:42.235378   57881 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 64444 <nil> <nil>}
I0917 21:20:42.235384   57881 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0917 21:20:42.372434   57881 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0917 21:20:42.372441   57881 ubuntu.go:71] root file system type: overlay
I0917 21:20:42.372563   57881 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0917 21:20:42.372699   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:42.430116   57881 main.go:141] libmachine: Using SSH client type: native
I0917 21:20:42.430445   57881 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 64444 <nil> <nil>}
I0917 21:20:42.430501   57881 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0917 21:20:42.577907   57881 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0917 21:20:42.577981   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:42.638059   57881 main.go:141] libmachine: Using SSH client type: native
I0917 21:20:42.638400   57881 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 64444 <nil> <nil>}
I0917 21:20:42.638412   57881 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0917 21:20:42.783863   57881 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0917 21:20:42.783873   57881 machine.go:91] provisioned docker machine in 4.443473912s
I0917 21:20:42.783898   57881 start.go:300] post-start starting for "minikube" (driver="docker")
I0917 21:20:42.783909   57881 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0917 21:20:42.783987   57881 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0917 21:20:42.784028   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:42.841594   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:20:42.939746   57881 ssh_runner.go:195] Run: cat /etc/os-release
I0917 21:20:42.944703   57881 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0917 21:20:42.944734   57881 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0917 21:20:42.944742   57881 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0917 21:20:42.944748   57881 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0917 21:20:42.944759   57881 filesync.go:126] Scanning /Users/apiiro/.minikube/addons for local assets ...
I0917 21:20:42.944872   57881 filesync.go:126] Scanning /Users/apiiro/.minikube/files for local assets ...
I0917 21:20:42.944938   57881 start.go:303] post-start completed in 161.012401ms
I0917 21:20:42.945320   57881 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0917 21:20:42.945366   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:43.003081   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:20:43.098686   57881 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0917 21:20:43.105545   57881 fix.go:56] fixHost completed within 5.836196178s
I0917 21:20:43.105567   57881 start.go:83] releasing machines lock for "minikube", held for 5.836230499s
I0917 21:20:43.105978   57881 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 21:20:43.163103   57881 ssh_runner.go:195] Run: cat /version.json
I0917 21:20:43.163195   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:43.164491   57881 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0917 21:20:43.165437   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:20:43.220799   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:20:43.221109   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:20:44.041179   57881 ssh_runner.go:195] Run: systemctl --version
I0917 21:20:44.051160   57881 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0917 21:20:44.057970   57881 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0917 21:20:44.079901   57881 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0917 21:20:44.079979   57881 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0917 21:20:44.089449   57881 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0917 21:20:44.089462   57881 start.go:466] detecting cgroup driver to use...
I0917 21:20:44.089762   57881 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0917 21:20:44.091547   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0917 21:20:44.108633   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0917 21:20:44.119661   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0917 21:20:44.130012   57881 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0917 21:20:44.130064   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0917 21:20:44.141047   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0917 21:20:44.151993   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0917 21:20:44.163322   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0917 21:20:44.173724   57881 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0917 21:20:44.184090   57881 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0917 21:20:44.194405   57881 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0917 21:20:44.205150   57881 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0917 21:20:44.213675   57881 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 21:20:44.287056   57881 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0917 21:20:44.374804   57881 start.go:466] detecting cgroup driver to use...
I0917 21:20:44.374818   57881 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0917 21:20:44.374890   57881 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0917 21:20:44.390712   57881 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0917 21:20:44.390797   57881 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0917 21:20:44.406988   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0917 21:20:44.425941   57881 ssh_runner.go:195] Run: which cri-dockerd
I0917 21:20:44.430202   57881 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0917 21:20:44.440626   57881 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0917 21:20:44.460841   57881 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0917 21:20:44.618665   57881 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0917 21:20:44.743859   57881 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0917 21:20:44.744140   57881 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0917 21:20:44.762704   57881 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 21:20:44.859921   57881 ssh_runner.go:195] Run: sudo systemctl restart docker
I0917 21:20:45.214166   57881 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0917 21:20:45.292561   57881 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0917 21:20:45.375541   57881 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0917 21:20:45.451318   57881 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 21:20:45.523686   57881 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0917 21:20:45.540218   57881 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 21:20:45.624674   57881 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0917 21:20:45.902772   57881 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0917 21:20:45.903299   57881 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0917 21:20:45.908588   57881 start.go:534] Will wait 60s for crictl version
I0917 21:20:45.908649   57881 ssh_runner.go:195] Run: which crictl
I0917 21:20:45.913151   57881 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0917 21:20:46.069313   57881 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0917 21:20:46.069426   57881 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0917 21:20:46.161540   57881 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0917 21:20:46.243177   57881 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I0917 21:20:46.244128   57881 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0917 21:20:46.427148   57881 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0917 21:20:46.427904   57881 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0917 21:20:46.432722   57881 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0917 21:20:46.444747   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 21:20:46.518898   57881 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0917 21:20:46.518967   57881 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0917 21:20:46.549421   57881 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0917 21:20:46.549938   57881 docker.go:566] Images already preloaded, skipping extraction
I0917 21:20:46.550356   57881 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0917 21:20:46.573179   57881 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0917 21:20:46.573714   57881 cache_images.go:84] Images are preloaded, skipping loading
I0917 21:20:46.574338   57881 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0917 21:20:46.872654   57881 cni.go:84] Creating CNI manager for ""
I0917 21:20:46.872683   57881 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 21:20:46.873548   57881 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0917 21:20:46.873573   57881 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0917 21:20:46.873806   57881 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0917 21:20:46.873885   57881 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0917 21:20:46.873979   57881 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0917 21:20:46.885367   57881 binaries.go:44] Found k8s binaries, skipping transfer
I0917 21:20:46.885488   57881 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0917 21:20:46.894941   57881 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0917 21:20:46.916320   57881 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0917 21:20:46.934390   57881 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0917 21:20:46.952849   57881 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0917 21:20:46.958268   57881 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0917 21:20:46.971149   57881 certs.go:56] Setting up /Users/apiiro/.minikube/profiles/minikube for IP: 192.168.49.2
I0917 21:20:46.971169   57881 certs.go:190] acquiring lock for shared ca certs: {Name:mk107fa356827842956eb2b3cfee6cb22d066cfb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 21:20:46.971575   57881 certs.go:199] skipping minikubeCA CA generation: /Users/apiiro/.minikube/ca.key
I0917 21:20:46.971923   57881 certs.go:199] skipping proxyClientCA CA generation: /Users/apiiro/.minikube/proxy-client-ca.key
I0917 21:20:46.973026   57881 certs.go:315] skipping minikube-user signed cert generation: /Users/apiiro/.minikube/profiles/minikube/client.key
I0917 21:20:46.973421   57881 certs.go:315] skipping minikube signed cert generation: /Users/apiiro/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0917 21:20:46.973887   57881 certs.go:315] skipping aggregator signed cert generation: /Users/apiiro/.minikube/profiles/minikube/proxy-client.key
I0917 21:20:46.974341   57881 certs.go:437] found cert: /Users/apiiro/.minikube/certs/Users/apiiro/.minikube/certs/ca-key.pem (1679 bytes)
I0917 21:20:46.974472   57881 certs.go:437] found cert: /Users/apiiro/.minikube/certs/Users/apiiro/.minikube/certs/ca.pem (1078 bytes)
I0917 21:20:46.974549   57881 certs.go:437] found cert: /Users/apiiro/.minikube/certs/Users/apiiro/.minikube/certs/cert.pem (1123 bytes)
I0917 21:20:46.974664   57881 certs.go:437] found cert: /Users/apiiro/.minikube/certs/Users/apiiro/.minikube/certs/key.pem (1675 bytes)
I0917 21:20:46.977847   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0917 21:20:47.002586   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0917 21:20:47.026068   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0917 21:20:47.048880   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0917 21:20:47.073166   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0917 21:20:47.099035   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0917 21:20:47.122048   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0917 21:20:47.145369   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0917 21:20:47.169146   57881 ssh_runner.go:362] scp /Users/apiiro/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0917 21:20:47.193594   57881 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0917 21:20:47.210898   57881 ssh_runner.go:195] Run: openssl version
I0917 21:20:47.221461   57881 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0917 21:20:47.233150   57881 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0917 21:20:47.238251   57881 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Sep 13 14:10 /usr/share/ca-certificates/minikubeCA.pem
I0917 21:20:47.238304   57881 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0917 21:20:47.245885   57881 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0917 21:20:47.257055   57881 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0917 21:20:47.262753   57881 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0917 21:20:47.271214   57881 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0917 21:20:47.279487   57881 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0917 21:20:47.288965   57881 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0917 21:20:47.298018   57881 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0917 21:20:47.308046   57881 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0917 21:20:47.317232   57881 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0917 21:20:47.317326   57881 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0917 21:20:47.342166   57881 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0917 21:20:47.351768   57881 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0917 21:20:47.352113   57881 kubeadm.go:636] restartCluster start
I0917 21:20:47.352172   57881 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0917 21:20:47.361762   57881 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0917 21:20:47.361827   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 21:20:47.424944   57881 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:59739"
I0917 21:20:47.424962   57881 kubeconfig.go:135] verify returned: got: 127.0.0.1:59739, want: 127.0.0.1:64443
I0917 21:20:47.425455   57881 lock.go:35] WriteFile acquiring /Users/apiiro/.kube/config: {Name:mkd0976f3c1d1cd629dc786c03d6e7ec797083bd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 21:20:47.437072   57881 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0917 21:20:47.447992   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:47.448062   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:47.461918   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:47.461925   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:47.461992   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:47.472981   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:47.973121   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:47.973184   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:47.985248   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:48.473256   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:48.473388   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:48.488496   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:48.973166   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:48.973319   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:48.988397   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:49.474333   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:49.474468   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:49.491017   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:49.973863   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:49.973986   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:49.989781   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:50.473415   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:50.473493   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:50.487061   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:50.973356   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:50.973468   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:50.987199   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:51.473599   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:51.473753   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:51.489114   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:51.974335   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:51.974528   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:51.992562   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:52.474492   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:52.474675   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:52.491520   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:52.973727   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:52.973830   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:52.988915   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:53.473664   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:53.473792   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:53.487642   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:53.974667   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:53.974909   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:53.992427   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:54.474227   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:54.474429   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:54.491423   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:54.974630   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:54.974756   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:54.988647   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:55.474419   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:55.474541   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:55.488146   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:55.974743   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:55.974923   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:55.991021   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:56.474619   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:56.474766   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:56.491475   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:56.974723   57881 api_server.go:166] Checking apiserver status ...
I0917 21:20:56.974837   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0917 21:20:56.990235   57881 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0917 21:20:57.449590   57881 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0917 21:20:57.449640   57881 kubeadm.go:1128] stopping kube-system containers ...
I0917 21:20:57.449740   57881 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0917 21:20:57.492235   57881 docker.go:462] Stopping containers: [20050096eea3 9e7ce176f03b a9e8ac9d179c 5198f25be542 028c841a8af8 8504a3126723 d2c6077374dc b519b0e078f4 90a88642fe5b e163d081f893 3e865fa541d1 b38d8c6f97a0 e90f0dedbd18 5b69f3b959bf f8be6957f748 3c9cf1c17f15 c7fbc69e8446 b61bc4040a29 4e84f7af77f7 72717dcd04a2 42ad3bf74d32 c445b1f881e7 d45e19d80c76 5dd7e701f851 799abd662a90 c35c727b1e98 a921eda53c9d]
I0917 21:20:57.492302   57881 ssh_runner.go:195] Run: docker stop 20050096eea3 9e7ce176f03b a9e8ac9d179c 5198f25be542 028c841a8af8 8504a3126723 d2c6077374dc b519b0e078f4 90a88642fe5b e163d081f893 3e865fa541d1 b38d8c6f97a0 e90f0dedbd18 5b69f3b959bf f8be6957f748 3c9cf1c17f15 c7fbc69e8446 b61bc4040a29 4e84f7af77f7 72717dcd04a2 42ad3bf74d32 c445b1f881e7 d45e19d80c76 5dd7e701f851 799abd662a90 c35c727b1e98 a921eda53c9d
I0917 21:20:57.520954   57881 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0917 21:20:57.534653   57881 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0917 21:20:57.545628   57881 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Sep 13 14:10 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Sep 13 14:10 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Sep 13 14:11 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Sep 13 14:10 /etc/kubernetes/scheduler.conf

I0917 21:20:57.545697   57881 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0917 21:20:57.556185   57881 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0917 21:20:57.567138   57881 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0917 21:20:57.577615   57881 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0917 21:20:57.577686   57881 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0917 21:20:57.587495   57881 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0917 21:20:57.598229   57881 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0917 21:20:57.598294   57881 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0917 21:20:57.609656   57881 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0917 21:20:57.619768   57881 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0917 21:20:57.619777   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0917 21:20:57.979999   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0917 21:20:58.477795   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0917 21:20:58.686341   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0917 21:20:58.746285   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0917 21:20:58.824816   57881 api_server.go:52] waiting for apiserver process to appear ...
I0917 21:20:58.824884   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:20:58.836976   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:20:59.349330   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:20:59.849003   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:21:00.348429   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:21:00.848630   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:21:01.348822   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:21:01.428522   57881 api_server.go:72] duration metric: took 2.603591715s to wait for apiserver process to appear ...
I0917 21:21:01.428530   57881 api_server.go:88] waiting for apiserver healthz status ...
I0917 21:21:01.428872   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:01.430976   57881 api_server.go:269] stopped: https://127.0.0.1:64443/healthz: Get "https://127.0.0.1:64443/healthz": EOF
I0917 21:21:01.430994   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:01.432158   57881 api_server.go:269] stopped: https://127.0.0.1:64443/healthz: Get "https://127.0.0.1:64443/healthz": EOF
I0917 21:21:01.932423   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:01.934789   57881 api_server.go:269] stopped: https://127.0.0.1:64443/healthz: Get "https://127.0.0.1:64443/healthz": EOF
I0917 21:21:02.432555   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:02.434183   57881 api_server.go:269] stopped: https://127.0.0.1:64443/healthz: Get "https://127.0.0.1:64443/healthz": EOF
I0917 21:21:02.932394   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:05.533476   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0917 21:21:05.533490   57881 api_server.go:103] status: https://127.0.0.1:64443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0917 21:21:05.533505   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:05.609356   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0917 21:21:05.609404   57881 api_server.go:103] status: https://127.0.0.1:64443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0917 21:21:05.932823   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:05.942294   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0917 21:21:05.942319   57881 api_server.go:103] status: https://127.0.0.1:64443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0917 21:21:06.432606   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:06.440895   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0917 21:21:06.440910   57881 api_server.go:103] status: https://127.0.0.1:64443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0917 21:21:06.933264   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:07.011378   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0917 21:21:07.011406   57881 api_server.go:103] status: https://127.0.0.1:64443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0917 21:21:07.432625   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:07.513098   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 200:
ok
I0917 21:21:07.530382   57881 api_server.go:141] control plane version: v1.27.4
I0917 21:21:07.530398   57881 api_server.go:131] duration metric: took 6.101644413s to wait for apiserver health ...
I0917 21:21:07.530405   57881 cni.go:84] Creating CNI manager for ""
I0917 21:21:07.530418   57881 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 21:21:07.550861   57881 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0917 21:21:07.571111   57881 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0917 21:21:07.708655   57881 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0917 21:21:07.807058   57881 system_pods.go:43] waiting for kube-system pods to appear ...
I0917 21:21:07.831515   57881 system_pods.go:59] 7 kube-system pods found
I0917 21:21:07.831536   57881 system_pods.go:61] "coredns-5d78c9869d-2s955" [f488606c-a79c-4463-a8e8-465fdc8da79b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0917 21:21:07.831546   57881 system_pods.go:61] "etcd-minikube" [e5092ff8-8bfc-4011-9830-08d7d77ffc80] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0917 21:21:07.831551   57881 system_pods.go:61] "kube-apiserver-minikube" [dfd6d148-22d4-437a-bf11-89968d07cff1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0917 21:21:07.831555   57881 system_pods.go:61] "kube-controller-manager-minikube" [b89e059f-f29d-4431-adc5-f29c202a270a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0917 21:21:07.831563   57881 system_pods.go:61] "kube-proxy-hjkp6" [167bb2a1-6ecd-4a25-bd6d-81231fcf09df] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0917 21:21:07.831570   57881 system_pods.go:61] "kube-scheduler-minikube" [54c6c22f-4488-4ac4-a63d-7541c6bd1840] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0917 21:21:07.831574   57881 system_pods.go:61] "storage-provisioner" [ee881f04-701f-4534-8bfc-a06bee865b3a] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0917 21:21:07.831579   57881 system_pods.go:74] duration metric: took 24.504872ms to wait for pod list to return data ...
I0917 21:21:07.831586   57881 node_conditions.go:102] verifying NodePressure condition ...
I0917 21:21:07.837164   57881 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0917 21:21:07.837183   57881 node_conditions.go:123] node cpu capacity is 8
I0917 21:21:07.837355   57881 node_conditions.go:105] duration metric: took 5.761483ms to run NodePressure ...
I0917 21:21:07.837650   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0917 21:21:08.731850   57881 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0917 21:21:08.811949   57881 ops.go:34] apiserver oom_adj: -16
I0917 21:21:08.811956   57881 kubeadm.go:640] restartCluster took 21.458748875s
I0917 21:21:08.811969   57881 kubeadm.go:406] StartCluster complete in 21.493659392s
I0917 21:21:08.812564   57881 settings.go:142] acquiring lock: {Name:mk60fd827562af7efc5fb2204644985ba3d93936 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 21:21:08.812767   57881 settings.go:150] Updating kubeconfig:  /Users/apiiro/.kube/config
I0917 21:21:08.813605   57881 lock.go:35] WriteFile acquiring /Users/apiiro/.kube/config: {Name:mkd0976f3c1d1cd629dc786c03d6e7ec797083bd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 21:21:08.814697   57881 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0917 21:21:08.815047   57881 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0917 21:21:08.815395   57881 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0917 21:21:08.815450   57881 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0917 21:21:08.815456   57881 addons.go:69] Setting dashboard=true in profile "minikube"
I0917 21:21:08.815463   57881 addons.go:231] Setting addon dashboard=true in "minikube"
W0917 21:21:08.815467   57881 addons.go:240] addon dashboard should already be in state true
I0917 21:21:08.815468   57881 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0917 21:21:08.815471   57881 addons.go:240] addon storage-provisioner should already be in state true
I0917 21:21:08.815472   57881 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0917 21:21:08.815494   57881 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0917 21:21:08.816730   57881 host.go:66] Checking if "minikube" exists ...
I0917 21:21:08.816732   57881 host.go:66] Checking if "minikube" exists ...
I0917 21:21:08.816956   57881 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 21:21:08.817090   57881 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 21:21:08.817147   57881 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 21:21:08.823621   57881 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0917 21:21:08.824119   57881 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0917 21:21:08.862843   57881 out.go:177] üîé  Verifying Kubernetes components...
I0917 21:21:08.902082   57881 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0917 21:21:08.939968   57881 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0917 21:21:08.958858   57881 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0917 21:21:08.934510   57881 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0917 21:21:08.977757   57881 addons.go:240] addon default-storageclass should already be in state true
I0917 21:21:08.977883   57881 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0917 21:21:08.995899   57881 host.go:66] Checking if "minikube" exists ...
I0917 21:21:09.014907   57881 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0917 21:21:09.014914   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0917 21:21:09.014991   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:21:09.015233   57881 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 21:21:09.034071   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0917 21:21:09.034081   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0917 21:21:09.034165   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:21:09.128470   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:21:09.128473   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:21:09.128583   57881 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0917 21:21:09.128590   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0917 21:21:09.128663   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 21:21:09.188147   57881 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64444 SSHKeyPath:/Users/apiiro/.minikube/machines/minikube/id_rsa Username:docker}
I0917 21:21:09.518421   57881 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0917 21:21:09.624058   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0917 21:21:09.624078   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0917 21:21:09.626867   57881 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0917 21:21:09.823136   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0917 21:21:09.823148   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0917 21:21:10.010062   57881 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.194953742s)
I0917 21:21:10.010154   57881 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.108016919s)
I0917 21:21:10.010250   57881 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0917 21:21:10.010273   57881 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 21:21:10.083473   57881 api_server.go:52] waiting for apiserver process to appear ...
I0917 21:21:10.083578   57881 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 21:21:10.120253   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0917 21:21:10.120274   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0917 21:21:10.223629   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0917 21:21:10.223643   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0917 21:21:10.321238   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0917 21:21:10.321247   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0917 21:21:10.410254   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0917 21:21:10.410268   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0917 21:21:10.437791   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0917 21:21:10.437804   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0917 21:21:10.528590   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0917 21:21:10.528599   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0917 21:21:10.619529   57881 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0917 21:21:10.619538   57881 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0917 21:21:10.643786   57881 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0917 21:21:12.719644   57881 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.201085878s)
I0917 21:21:12.719666   57881 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.092691768s)
I0917 21:21:12.719805   57881 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.636130467s)
I0917 21:21:12.719819   57881 api_server.go:72] duration metric: took 3.895546938s to wait for apiserver process to appear ...
I0917 21:21:12.719822   57881 api_server.go:88] waiting for apiserver healthz status ...
I0917 21:21:12.719841   57881 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64443/healthz ...
I0917 21:21:12.728100   57881 api_server.go:279] https://127.0.0.1:64443/healthz returned 200:
ok
I0917 21:21:12.729982   57881 api_server.go:141] control plane version: v1.27.4
I0917 21:21:12.729990   57881 api_server.go:131] duration metric: took 10.163513ms to wait for apiserver health ...
I0917 21:21:12.729995   57881 system_pods.go:43] waiting for kube-system pods to appear ...
I0917 21:21:12.736324   57881 system_pods.go:59] 7 kube-system pods found
I0917 21:21:12.736335   57881 system_pods.go:61] "coredns-5d78c9869d-2s955" [f488606c-a79c-4463-a8e8-465fdc8da79b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0917 21:21:12.736341   57881 system_pods.go:61] "etcd-minikube" [e5092ff8-8bfc-4011-9830-08d7d77ffc80] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0917 21:21:12.736346   57881 system_pods.go:61] "kube-apiserver-minikube" [dfd6d148-22d4-437a-bf11-89968d07cff1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0917 21:21:12.736352   57881 system_pods.go:61] "kube-controller-manager-minikube" [b89e059f-f29d-4431-adc5-f29c202a270a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0917 21:21:12.736355   57881 system_pods.go:61] "kube-proxy-hjkp6" [167bb2a1-6ecd-4a25-bd6d-81231fcf09df] Running
I0917 21:21:12.736361   57881 system_pods.go:61] "kube-scheduler-minikube" [54c6c22f-4488-4ac4-a63d-7541c6bd1840] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0917 21:21:12.736364   57881 system_pods.go:61] "storage-provisioner" [ee881f04-701f-4534-8bfc-a06bee865b3a] Running
I0917 21:21:12.736369   57881 system_pods.go:74] duration metric: took 6.370413ms to wait for pod list to return data ...
I0917 21:21:12.736373   57881 kubeadm.go:581] duration metric: took 3.912103078s to wait for : map[apiserver:true system_pods:true] ...
I0917 21:21:12.736381   57881 node_conditions.go:102] verifying NodePressure condition ...
I0917 21:21:12.739843   57881 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0917 21:21:12.739851   57881 node_conditions.go:123] node cpu capacity is 8
I0917 21:21:12.739857   57881 node_conditions.go:105] duration metric: took 3.473458ms to run NodePressure ...
I0917 21:21:12.739865   57881 start.go:228] waiting for startup goroutines ...
I0917 21:21:12.977894   57881 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.334021393s)
I0917 21:21:12.997180   57881 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0917 21:21:13.016302   57881 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0917 21:21:13.035302   57881 addons.go:502] enable addons completed in 4.220944097s: enabled=[storage-provisioner default-storageclass dashboard]
I0917 21:21:13.035328   57881 start.go:233] waiting for cluster config update ...
I0917 21:21:13.035342   57881 start.go:242] writing updated cluster config ...
I0917 21:21:13.036675   57881 ssh_runner.go:195] Run: rm -f paused
I0917 21:21:13.089086   57881 start.go:600] kubectl: 1.28.1, cluster: 1.27.4 (minor skew: 1)
I0917 21:21:13.127280   57881 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Sep 17 18:20:45 minikube dockerd[808]: time="2023-09-17T18:20:45.177703808Z" level=info msg="Daemon has completed initialization"
Sep 17 18:20:45 minikube dockerd[808]: time="2023-09-17T18:20:45.207829237Z" level=info msg="API listen on /var/run/docker.sock"
Sep 17 18:20:45 minikube dockerd[808]: time="2023-09-17T18:20:45.207990790Z" level=info msg="API listen on [::]:2376"
Sep 17 18:20:45 minikube systemd[1]: Started Docker Application Container Engine.
Sep 17 18:20:45 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Start docker client with request timeout 0s"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Loaded network plugin cni"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Docker Info: &{ID:240486bb-ae5f-4e1b-996c-5a512b979765 Containers:39 ContainersRunning:0 ContainersPaused:0 ContainersStopped:39 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:35 SystemTime:2023-09-17T18:20:45.886215786Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:2 NEventsListener:0 KernelVersion:5.15.49-linuxkit-pr OperatingSystem:Ubuntu 22.04.2 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0001a61c0 NCPU:8 MemTotal:8240328704 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Setting cgroupDriver cgroupfs"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 17 18:20:45 minikube cri-dockerd[1041]: time="2023-09-17T18:20:45Z" level=info msg="Start cri-dockerd grpc backend"
Sep 17 18:20:45 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-5c5cfc8747-p852n_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fdb4d423f3f4dee6348a087b0794b4ff75d1e5ae94eeda153db88b6e60b303df\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-5c5cfc8747-p852n_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"562941a7514e1f3fab4a9b7bb7fd4920aebd7bfc2653cf568b326b8975394934\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-59d4768566-nwvfr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a42ed519d640b4361eaa888e0327fc75a52798053866f86bb810ab69a783f496\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-59d4768566-nwvfr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"89b800cd4f569fc4cadd4bf09baee060983c48f234aabff4c162266a77342a72\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-2s955_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"90a88642fe5b9162c60a17b405a656772a7feab9a59dbddcff8d0afe9b0d8037\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-2s955_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c7fbc69e84466c1aa79c2b7e59bc45132121fdcb9932e47b84959e1b11d80006\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5dd9cbfd69-snftb_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9da9b03e65b40410107a47887f3b147072ad3454a6cd72179dbe7aa9e5028816\""
Sep 17 18:20:59 minikube cri-dockerd[1041]: time="2023-09-17T18:20:59Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5dd9cbfd69-snftb_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ca8ef8102218a7d893a89fe46db0d218c900c4239980ee5467127d890191fae3\""
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a921eda53c9dadb7ef4baf93699c33b48250445ff025f528a01b66dbebaa9021\". Proceed without further sandbox information."
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"c35c727b1e98bc1d5d58732390339911e5f4aabb8d5f971605e3c7d8f762ae8b\". Proceed without further sandbox information."
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"799abd662a90c35ca7689c97371bca71b1a98adb5ad1296c50b376daf7c97ab7\". Proceed without further sandbox information."
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"5dd7e701f851384a0587821f2ba7d35d7fe276b84ffc01d3af3e89d8fca5b0e7\". Proceed without further sandbox information."
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-5c5cfc8747-p852n_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fdb4d423f3f4dee6348a087b0794b4ff75d1e5ae94eeda153db88b6e60b303df\""
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-2s955_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"90a88642fe5b9162c60a17b405a656772a7feab9a59dbddcff8d0afe9b0d8037\""
Sep 17 18:21:00 minikube cri-dockerd[1041]: time="2023-09-17T18:21:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c0a1a06c9f71e3e7f7f1647d7f741f23b5a8a4f6ce9b47d7ec561724233206b6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:01 minikube cri-dockerd[1041]: time="2023-09-17T18:21:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/72fe1232b1ef04d60516dc45bf39c6296592420c1d277bf3e70c9c86651338bf/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:01 minikube cri-dockerd[1041]: time="2023-09-17T18:21:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1a091e627091b96becd6b80c136bfd368a94d154355a26d1b65c8d27a5b9883/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:01 minikube cri-dockerd[1041]: time="2023-09-17T18:21:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5dd9cbfd69-snftb_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9da9b03e65b40410107a47887f3b147072ad3454a6cd72179dbe7aa9e5028816\""
Sep 17 18:21:01 minikube cri-dockerd[1041]: time="2023-09-17T18:21:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1715a48966538933e24acbfc96cf56a64005a829a0bb3db6fe00678322360d3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:01 minikube cri-dockerd[1041]: time="2023-09-17T18:21:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-59d4768566-nwvfr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a42ed519d640b4361eaa888e0327fc75a52798053866f86bb810ab69a783f496\""
Sep 17 18:21:05 minikube cri-dockerd[1041]: time="2023-09-17T18:21:05Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Sep 17 18:21:07 minikube cri-dockerd[1041]: time="2023-09-17T18:21:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/25a832b70ea114eef931ba4798605528046ec24917ad3f099dc8dd4211d033c8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:07 minikube cri-dockerd[1041]: time="2023-09-17T18:21:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9715556cb4ed2b33b99e78eb92928cf1b808271604060e1e6cd1b1eeb2d059ab/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:07 minikube cri-dockerd[1041]: time="2023-09-17T18:21:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/02bac92e509fd3e623b0a470888387599814314523d130ac9e53beb9cd6362a3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 18:21:07 minikube cri-dockerd[1041]: time="2023-09-17T18:21:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c6cc5a97b54672c66d873aaa33b5997e2b5cec37f56fdc1f7e91cd80b3fd51c7/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 18:21:07 minikube cri-dockerd[1041]: time="2023-09-17T18:21:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aeb9f4b3c14f53149a44a227f05572b53f51d8c1e06b9dab5cb7cd1d88e280b3/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 18:21:07 minikube cri-dockerd[1041]: time="2023-09-17T18:21:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c0ff3985581f232e53432433cec19f06497e73abab4f87ab449eda4820ade8c0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 18:21:39 minikube dockerd[808]: time="2023-09-17T18:21:39.054652083Z" level=info msg="ignoring event" container=eedf7ebbf07ff8273185f7341bb61d745c168fa916348ec88cfae9191e4716fe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 18:21:40 minikube dockerd[808]: time="2023-09-17T18:21:40.133362046Z" level=info msg="ignoring event" container=d22d55502196cfcf51b3f066903f770ece588294b523adcbc3409b3c2bbe20b8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 18:24:21 minikube cri-dockerd[1041]: time="2023-09-17T18:24:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/49dd448a4622aba9257903eab41f388e42b44aaac53607bbb179c3b670c290c6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 18:24:24 minikube dockerd[808]: time="2023-09-17T18:24:24.290320752Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:24:24 minikube dockerd[808]: time="2023-09-17T18:24:24.290412710Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 18:24:39 minikube dockerd[808]: time="2023-09-17T18:24:39.370589695Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:24:39 minikube dockerd[808]: time="2023-09-17T18:24:39.370691660Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 18:25:10 minikube dockerd[808]: time="2023-09-17T18:25:10.197711800Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:25:10 minikube dockerd[808]: time="2023-09-17T18:25:10.197794163Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 18:25:52 minikube dockerd[808]: time="2023-09-17T18:25:52.319284222Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:25:52 minikube dockerd[808]: time="2023-09-17T18:25:52.319374227Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 18:27:23 minikube dockerd[808]: time="2023-09-17T18:27:23.936093829Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:27:23 minikube dockerd[808]: time="2023-09-17T18:27:23.936322946Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 18:30:10 minikube dockerd[808]: time="2023-09-17T18:30:10.589456463Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:30:10 minikube dockerd[808]: time="2023-09-17T18:30:10.589559716Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 18:35:20 minikube dockerd[808]: time="2023-09-17T18:35:20.470029582Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 18:35:20 minikube dockerd[808]: time="2023-09-17T18:35:20.470129845Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
7948ab3aef7bb       6e38f40d628db       15 minutes ago      Running             storage-provisioner         5                   9715556cb4ed2       storage-provisioner
cc7a564bafd8d       07655ddf2eebe       15 minutes ago      Running             kubernetes-dashboard        3                   aeb9f4b3c14f5       kubernetes-dashboard-5c5cfc8747-p852n
d22d55502196c       6e38f40d628db       16 minutes ago      Exited              storage-provisioner         4                   9715556cb4ed2       storage-provisioner
e45d0b800d536       9056ab77afb8e       16 minutes ago      Running             echo-server                 2                   02bac92e509fd       hello-minikube-59d4768566-nwvfr
eedf7ebbf07ff       07655ddf2eebe       16 minutes ago      Exited              kubernetes-dashboard        2                   aeb9f4b3c14f5       kubernetes-dashboard-5c5cfc8747-p852n
334bbc0583d2e       6848d7eda0341       16 minutes ago      Running             kube-proxy                  2                   25a832b70ea11       kube-proxy-hjkp6
7488ae057d86d       115053965e86b       16 minutes ago      Running             dashboard-metrics-scraper   2                   c6cc5a97b5467       dashboard-metrics-scraper-5dd9cbfd69-snftb
9da8b898c3fe6       ead0a4a53df89       16 minutes ago      Running             coredns                     2                   c0ff3985581f2       coredns-5d78c9869d-2s955
f4d4ef90f77d6       e7972205b6614       16 minutes ago      Running             kube-apiserver              2                   c0a1a06c9f71e       kube-apiserver-minikube
f2487cd3e7aae       98ef2570f3cde       16 minutes ago      Running             kube-scheduler              2                   d1715a4896653       kube-scheduler-minikube
60fd3eb5059d9       f466468864b7a       16 minutes ago      Running             kube-controller-manager     2                   d1a091e627091       kube-controller-manager-minikube
c476ab78a7ace       86b6af7dd652c       16 minutes ago      Running             etcd                        2                   72fe1232b1ef0       etcd-minikube
ef7d8363f2d6c       115053965e86b       4 days ago          Exited              dashboard-metrics-scraper   1                   9da9b03e65b40       dashboard-metrics-scraper-5dd9cbfd69-snftb
a9cfd0d53e340       9056ab77afb8e       4 days ago          Exited              echo-server                 1                   a42ed519d640b       hello-minikube-59d4768566-nwvfr
9e7ce176f03ba       ead0a4a53df89       4 days ago          Exited              coredns                     1                   90a88642fe5b9       coredns-5d78c9869d-2s955
a9e8ac9d179c9       f466468864b7a       4 days ago          Exited              kube-controller-manager     1                   b38d8c6f97a0b       kube-controller-manager-minikube
028c841a8af87       98ef2570f3cde       4 days ago          Exited              kube-scheduler              1                   e90f0dedbd185       kube-scheduler-minikube
8504a31267232       e7972205b6614       4 days ago          Exited              kube-apiserver              1                   3e865fa541d19       kube-apiserver-minikube
d2c6077374dc3       6848d7eda0341       4 days ago          Exited              kube-proxy                  1                   e163d081f893b       kube-proxy-hjkp6
b519b0e078f49       86b6af7dd652c       4 days ago          Exited              etcd                        1                   f8be6957f748a       etcd-minikube

* 
* ==> coredns [9da8b898c3fe] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:45025 - 22799 "HINFO IN 1498210615360356912.9016959264191247513. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.183862686s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> coredns [9e7ce176f03b] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:56800 - 355 "HINFO IN 8789990144321053783.5843357780766349714. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.082660752s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_09_13T17_11_03_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 13 Sep 2023 14:11:00 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 17 Sep 2023 18:37:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 17 Sep 2023 18:36:22 +0000   Wed, 13 Sep 2023 15:19:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 17 Sep 2023 18:36:22 +0000   Wed, 13 Sep 2023 15:19:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 17 Sep 2023 18:36:22 +0000   Wed, 13 Sep 2023 15:19:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 17 Sep 2023 18:36:22 +0000   Wed, 13 Sep 2023 15:19:48 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             8047196Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             8047196Ki
  pods:               110
System Info:
  Machine ID:                 6cf1630a5a10498c92e9da07c4ac520e
  System UUID:                6cf1630a5a10498c92e9da07c4ac520e
  Boot ID:                    078c38a9-2c3e-4a0a-aa6b-55dc294f74b0
  Kernel Version:             5.15.49-linuxkit-pr
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-minikube-59d4768566-nwvfr               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  default                     myserverapp-deployment-7d8767bf8b-wc4rx       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13m
  kube-system                 coredns-5d78c9869d-2s955                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     4d4h
  kube-system                 etcd-minikube                                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         4d4h
  kube-system                 kube-apiserver-minikube                       250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  kube-system                 kube-controller-manager-minikube              200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  kube-system                 kube-proxy-hjkp6                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  kube-system                 kube-scheduler-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-snftb    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-p852n         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d4h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 16m                  kube-proxy       
  Normal  Starting                 4d4h                 kube-proxy       
  Normal  Starting                 4d3h                 kube-proxy       
  Normal  NodeHasNoDiskPressure    4d4h (x8 over 4d4h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 4d4h                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  4d4h (x8 over 4d4h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasSufficientPID     4d4h (x7 over 4d4h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  4d4h                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeAllocatableEnforced  4d4h                 kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 4d4h                 kubelet          Starting kubelet.
  Normal  NodeNotReady             4d4h                 kubelet          Node minikube status is now: NodeNotReady
  Normal  RegisteredNode           4d4h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  RegisteredNode           4d3h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeNotReady             4d3h                 node-controller  Node minikube status is now: NodeNotReady
  Normal  NodeHasSufficientPID     4d3h (x2 over 4d4h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                4d3h (x2 over 4d4h)  kubelet          Node minikube status is now: NodeReady
  Normal  NodeHasNoDiskPressure    4d3h (x2 over 4d4h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  4d3h (x2 over 4d4h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  Starting                 16m                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  16m                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  16m (x8 over 16m)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    16m (x8 over 16m)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     16m (x7 over 16m)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           16m                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Sep17 13:09] Hangcheck: hangcheck value past margin!
[Sep17 14:03] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:
[  +0.000212] Hangcheck: hangcheck value past margin!
[  +0.011314] rcu: 	7-...!: (0 ticks this GP) idle=19d/1/0x4000000000000000 softirq=785364/785364 fqs=1 
[  +0.000008] 	(detected by 4, t=3233415 jiffies, g=2688397, q=35)
[  +0.349357] NMI backtrace for cpu 7
[  +0.000004] CPU: 7 PID: 141208 Comm: lifecycle-serve Tainted: G           O    T 5.15.49-linuxkit-pr #1
[  +0.000011] RIP: 0010:native_apic_mem_write+0x8/0x9
[  +0.000008] Code: 48 c7 c0 f0 bd 01 00 48 8b 14 fd 80 7a 30 92 8b 3c 02 0f ae f0 0f ae e8 ba 00 08 00 00 e9 5c fa ff ff 89 ff 89 b7 00 d0 5f ff <c3> 89 ff 8b 87 00 d0 5f ff c3 0f 1f 44 00 00 b8 01 00 00 00 c3 0f
[  +0.000002] RSP: 0018:ffffaf3e40284fd8 EFLAGS: 00010046
[  +0.000002] RAX: ffffffff9104c819 RBX: 0000000000000000 RCX: ffffffff91c01050
[  +0.000001] RDX: ffff8f8ac1ed2080 RSI: 0000000000000000 RDI: 00000000000000b0
[  +0.000001] RBP: ffffaf3e40593b58 R08: 0000000000000000 R09: 0000000000000000
[  +0.000002] R10: 0000000000000000 R11: ffffaf3e40284ff8 R12: 0000000000000000
[  +0.000001] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
[  +0.000001] FS:  00000000016ad6e0(0000) GS:ffff8f8bf19c0000(0000) knlGS:0000000000000000
[  +0.000002] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000002] CR2: 000000c00014a000 CR3: 000000011d020001 CR4: 00000000003706a0
[  +0.000001] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
[  +0.000001] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
[  +0.000001] Call Trace:
[  +0.000015]  <IRQ>
[  +0.000019]  __sysvec_apic_timer_interrupt+0x1c/0xdb
[  +0.000007]  sysvec_apic_timer_interrupt+0x61/0x7d
[  +0.000005]  </IRQ>
[  +0.000001]  <TASK>
[  +0.000001]  asm_sysvec_apic_timer_interrupt+0x12/0x20
[  +0.000004] RIP: 0010:check_vma_flags+0xde/0xe7
[  +0.000088] Code: 80 e3 10 74 20 41 80 e4 10 74 1a 31 d2 45 85 ed 44 89 f6 0f 95 c2 e8 f6 fe ff ff 41 89 c0 31 c0 45 84 c0 75 05 b8 f2 ff ff ff <5a> 5b 41 5c 41 5d 41 5e c3 48 89 f8 0f 1f 40 00 48 ba 00 f0 ff ff
[  +0.000002] RSP: 0018:ffffaf3e40593c00 EFLAGS: 00000202
[  +0.000003] RAX: 0000000000000000 RBX: 0000000000002017 RCX: 0000000000002010
[  +0.000008] RDX: 0000000000000001 RSI: 0000000000000001 RDI: ffff8f8b76a63c38
[  +0.000001] RBP: 0000000000000000 R08: 0000000000000001 R09: 0000000000000000
[  +0.000002] R10: 0000000000000001 R11: 000000000000b412 R12: 0000000000118173
[  +0.000001] R13: 0000000000002000 R14: 0000000000000001 R15: 0000000000000000
[  +0.001035]  __get_user_pages+0x435/0x4bd
[  +0.000011]  __get_user_pages_remote+0xe3/0x206
[  +0.000012]  get_arg_page+0x58/0xae
[  +0.000595]  copy_string_kernel+0xb1/0xf8
[  +0.000004]  load_script+0x151/0x191
[  +0.000039]  bprm_execve+0x258/0x57f
[  +0.000003]  do_execveat_common+0x1b2/0x1d7
[  +0.000003]  __x64_sys_execve+0x3e/0x4b
[  +0.000002]  do_syscall_64+0x72/0x88
[  +0.000029]  ? do_syscall_64+0xe/0x88
[  +0.000002]  entry_SYSCALL_64_after_hwframe+0x44/0xae
[  +0.000004] RIP: 0033:0x403c8e
[  +0.000033] Code: 48 89 6c 24 38 48 8d 6c 24 38 e8 0d 00 00 00 48 8b 6c 24 38 48 83 c4 40 c3 cc cc cc 49 89 f2 48 89 fa 48 89 ce 48 89 df 0f 05 <48> 3d 01 f0 ff ff 76 15 48 f7 d8 48 89 c1 48 c7 c0 ff ff ff ff 48
[  +0.000002] RSP: 002b:000000c000daf4a0 EFLAGS: 00000202 ORIG_RAX: 000000000000003b
[  +0.000003] RAX: ffffffffffffffda RBX: 000000c0003b6378 RCX: 0000000000403c8e
[  +0.000001] RDX: 000000c000f30460 RSI: 000000c000f54640 RDI: 000000c0003b6378
[  +0.000001] RBP: 000000c000daf4e0 R08: 0000000000000000 R09: 0000000000000000
[  +0.000002] R10: 0000000000000000 R11: 0000000000000202 R12: 00000000004bca12
[  +0.000001] R13: 0000000000000029 R14: 000000c000028b60 R15: 000000000167ea60
[  +0.000003]  </TASK>
[Sep17 14:39] Hangcheck: hangcheck value past margin!
[Sep17 16:33] Hangcheck: hangcheck value past margin!
[Sep17 17:21] Hangcheck: hangcheck value past margin!
[Sep17 17:25] Hangcheck: hangcheck value past margin!
[Sep17 17:49] Hangcheck: hangcheck value past margin!

* 
* ==> etcd [b519b0e078f4] <==
* {"level":"info","ts":"2023-09-13T14:40:25.181Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-09-13T14:40:25.256Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-09-13T14:40:25.260Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.008255ms"}
{"level":"info","ts":"2023-09-13T14:40:25.279Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2023-09-13T14:40:25.385Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":2251}
{"level":"info","ts":"2023-09-13T14:40:25.385Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2023-09-13T14:40:25.385Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2023-09-13T14:40:25.385Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 2251, applied: 0, lastindex: 2251, lastterm: 2]"}
{"level":"warn","ts":"2023-09-13T14:40:25.390Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-09-13T14:40:25.456Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1446}
{"level":"info","ts":"2023-09-13T14:40:25.463Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1886}
{"level":"info","ts":"2023-09-13T14:40:25.468Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-09-13T14:40:25.471Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-09-13T14:40:25.471Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-09-13T14:40:25.471Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-09-13T14:40:25.474Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2023-09-13T14:40:25.474Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-09-13T14:40:25.475Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-09-13T14:40:25.475Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-09-13T14:40:25.475Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-09-13T14:40:25.475Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-09-13T14:40:25.475Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-09-13T14:40:25.475Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-09-13T14:40:25.478Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-09-13T14:40:25.478Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-09-13T14:40:25.478Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-09-13T14:40:25.478Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-09-13T14:40:25.478Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2023-09-13T14:40:26.786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-09-13T14:40:26.788Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-09-13T14:40:26.788Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-09-13T14:40:26.788Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-09-13T14:40:26.788Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-09-13T14:40:26.788Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-09-13T14:40:26.789Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-09-13T14:40:26.789Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-09-13T14:50:26.808Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2229}
{"level":"info","ts":"2023-09-13T14:50:26.823Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2229,"took":"14.576408ms","hash":367415820}
{"level":"info","ts":"2023-09-13T14:50:26.823Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":367415820,"revision":2229,"compact-revision":1446}
{"level":"info","ts":"2023-09-13T14:55:26.790Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2468}
{"level":"info","ts":"2023-09-13T14:55:26.790Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2468,"took":"649.644¬µs","hash":3298040993}
{"level":"info","ts":"2023-09-13T14:55:26.790Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3298040993,"revision":2468,"compact-revision":2229}
{"level":"info","ts":"2023-09-13T15:00:26.794Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2708}
{"level":"info","ts":"2023-09-13T15:00:26.795Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2708,"took":"523.652¬µs","hash":2521464163}
{"level":"info","ts":"2023-09-13T15:00:26.795Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2521464163,"revision":2708,"compact-revision":2468}
{"level":"info","ts":"2023-09-13T15:05:26.800Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2949}
{"level":"info","ts":"2023-09-13T15:05:26.800Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2949,"took":"564.452¬µs","hash":1136943035}
{"level":"info","ts":"2023-09-13T15:05:26.801Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1136943035,"revision":2949,"compact-revision":2708}
{"level":"info","ts":"2023-09-13T15:10:26.819Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3188}
{"level":"info","ts":"2023-09-13T15:10:26.820Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3188,"took":"773.723¬µs","hash":121095702}
{"level":"info","ts":"2023-09-13T15:10:26.820Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":121095702,"revision":3188,"compact-revision":2949}
{"level":"info","ts":"2023-09-13T15:19:37.603Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3429}
{"level":"info","ts":"2023-09-13T15:19:37.609Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3429,"took":"3.44231ms","hash":2995831405}
{"level":"info","ts":"2023-09-13T15:19:37.609Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2995831405,"revision":3429,"compact-revision":3188}

* 
* ==> etcd [c476ab78a7ac] <==
* {"level":"info","ts":"2023-09-17T18:21:01.827Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-09-17T18:21:01.830Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-09-17T18:21:01.830Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-09-17T18:21:01.830Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-09-17T18:21:01.835Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-09-17T18:21:01.835Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-09-17T18:21:01.907Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"70.990879ms"}
{"level":"info","ts":"2023-09-17T18:21:02.112Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2023-09-17T18:21:02.306Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":4436}
{"level":"info","ts":"2023-09-17T18:21:02.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2023-09-17T18:21:02.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2023-09-17T18:21:02.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 4436, applied: 0, lastindex: 4436, lastterm: 3]"}
{"level":"warn","ts":"2023-09-17T18:21:02.317Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-09-17T18:21:02.317Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":3429}
{"level":"info","ts":"2023-09-17T18:21:02.330Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":3661}
{"level":"info","ts":"2023-09-17T18:21:02.331Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-09-17T18:21:02.334Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-09-17T18:21:02.335Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-09-17T18:21:02.335Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-09-17T18:21:02.336Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2023-09-17T18:21:02.336Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-09-17T18:21:02.336Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-09-17T18:21:02.337Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-09-17T18:21:02.338Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-09-17T18:21:02.338Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-09-17T18:21:02.339Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-09-17T18:21:02.339Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-09-17T18:21:02.404Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-09-17T18:21:02.405Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-09-17T18:21:02.405Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-09-17T18:21:02.405Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-09-17T18:21:02.405Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2023-09-17T18:21:03.316Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-09-17T18:21:03.358Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-09-17T18:21:03.359Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-09-17T18:21:03.358Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-09-17T18:21:03.360Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-09-17T18:21:03.360Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-09-17T18:21:03.362Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-09-17T18:21:03.362Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-09-17T18:31:03.437Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4051}
{"level":"info","ts":"2023-09-17T18:31:03.452Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4051,"took":"14.200178ms","hash":1719681720}
{"level":"info","ts":"2023-09-17T18:31:03.452Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1719681720,"revision":4051,"compact-revision":3429}
{"level":"info","ts":"2023-09-17T18:36:03.450Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4308}
{"level":"info","ts":"2023-09-17T18:36:03.450Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4308,"took":"663.16¬µs","hash":1215488367}
{"level":"info","ts":"2023-09-17T18:36:03.450Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1215488367,"revision":4308,"compact-revision":4051}

* 
* ==> kernel <==
*  18:37:40 up 4 days, 10:17,  0 users,  load average: 0.26, 0.22, 0.22
Linux minikube 5.15.49-linuxkit-pr #1 SMP Thu May 25 07:17:40 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [8504a3126723] <==
* I0913 14:40:28.365804       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0913 14:40:28.365899       1 aggregator.go:150] waiting for initial CRD sync...
I0913 14:40:28.366162       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0913 14:40:28.366176       1 controller.go:85] Starting OpenAPI controller
I0913 14:40:28.366182       1 controller.go:121] Starting legacy_token_tracking_controller
I0913 14:40:28.366189       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0913 14:40:28.366190       1 controller.go:85] Starting OpenAPI V3 controller
I0913 14:40:28.366194       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0913 14:40:28.366201       1 naming_controller.go:291] Starting NamingConditionController
I0913 14:40:28.366211       1 establishing_controller.go:76] Starting EstablishingController
I0913 14:40:28.366245       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0913 14:40:28.366254       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0913 14:40:28.366335       1 crd_finalizer.go:266] Starting CRDFinalizer
I0913 14:40:28.366418       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0913 14:40:28.366444       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0913 14:40:28.366458       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0913 14:40:28.366467       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0913 14:40:28.366474       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0913 14:40:28.366472       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0913 14:40:28.366485       1 controller.go:83] Starting OpenAPI AggregationController
I0913 14:40:28.366519       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0913 14:40:28.366529       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0913 14:40:28.366533       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0913 14:40:28.366537       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0913 14:40:28.366549       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0913 14:40:28.366563       1 available_controller.go:423] Starting AvailableConditionController
I0913 14:40:28.366593       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0913 14:40:28.366926       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0913 14:40:28.366580       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0913 14:40:28.366588       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0913 14:40:28.467897       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0913 14:40:28.468289       1 shared_informer.go:318] Caches are synced for configmaps
I0913 14:40:28.468343       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0913 14:40:28.470970       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0913 14:40:28.471019       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0913 14:40:28.471422       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0913 14:40:28.471834       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0913 14:40:28.472203       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0913 14:40:28.473141       1 aggregator.go:152] initial CRD sync complete...
I0913 14:40:28.473178       1 autoregister_controller.go:141] Starting autoregister controller
I0913 14:40:28.473184       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0913 14:40:28.473189       1 cache.go:39] Caches are synced for autoregister controller
I0913 14:40:28.488123       1 shared_informer.go:318] Caches are synced for node_authorizer
I0913 14:40:29.175030       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
E0913 14:40:29.258193       1 storage.go:470] Address {10.244.0.3  0xc00b1fe870 0xc00b1c1110} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.8}] vs 10.244.0.3 (kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-snftb))
E0913 14:40:29.258303       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.3  0xc00b1fe870 0xc00b1c1110}] [] [{ 8000 TCP <nil>}]}
I0913 14:40:29.371039       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0913 14:40:31.607836       1 controller.go:624] quota admission added evaluator for: endpoints
I0913 14:40:41.175771       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0913 15:19:37.603017       1 trace.go:219] Trace[190862176]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:495cc50c-6b92-480b-b916-f8d742262289,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (13-Sep-2023 15:19:37.059) (total time: 520ms):
Trace[190862176]: ["GuaranteedUpdate etcd3" audit-id:495cc50c-6b92-480b-b916-f8d742262289,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 520ms (15:19:37.059)
Trace[190862176]:  ---"Txn call completed" 517ms (15:19:37.601)]
Trace[190862176]: [520.399418ms] [520.399418ms] END
I0913 15:19:37.689535       1 trace.go:219] Trace[849417407]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2d004eda-d933-4f82-984f-f4a0dcf563e3,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (13-Sep-2023 15:19:37.073) (total time: 592ms):
Trace[849417407]: ---"About to write a response" 592ms (15:19:37.689)
Trace[849417407]: [592.685902ms] [592.685902ms] END
I0913 15:19:37.689806       1 trace.go:219] Trace[1666638207]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a273f544-fc34-4319-a1a5-55ad71672ea3,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kube-controller-manager/v1.27.4 (linux/amd64) kubernetes/fa3d799/system:serviceaccount:kube-system:node-controller,verb:PUT (13-Sep-2023 15:19:37.167) (total time: 522ms):
Trace[1666638207]: ---"limitedReadBody succeeded" len:4639 320ms (15:19:37.487)
Trace[1666638207]: ---"Writing http response done" 86ms (15:19:37.689)
Trace[1666638207]: [522.761026ms] [522.761026ms] END

* 
* ==> kube-apiserver [f4d4ef90f77d] <==
* I0917 18:21:05.509194       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 18:21:05.509671       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0917 18:21:05.510088       1 secure_serving.go:210] Serving securely on [::]:8443
I0917 18:21:05.510146       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0917 18:21:05.510246       1 aggregator.go:150] waiting for initial CRD sync...
I0917 18:21:05.510439       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0917 18:21:05.510503       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0917 18:21:05.510647       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0917 18:21:05.510725       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0917 18:21:05.510806       1 controller.go:85] Starting OpenAPI controller
I0917 18:21:05.510894       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0917 18:21:05.510903       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0917 18:21:05.510662       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0917 18:21:05.510457       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0917 18:21:05.511246       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0917 18:21:05.511798       1 available_controller.go:423] Starting AvailableConditionController
I0917 18:21:05.511833       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0917 18:21:05.511851       1 controller.go:83] Starting OpenAPI AggregationController
I0917 18:21:05.512011       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0917 18:21:05.510670       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0917 18:21:05.512298       1 controller.go:121] Starting legacy_token_tracking_controller
I0917 18:21:05.512330       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0917 18:21:05.513111       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0917 18:21:05.513151       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0917 18:21:05.513171       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0917 18:21:05.513355       1 controller.go:85] Starting OpenAPI V3 controller
I0917 18:21:05.513437       1 naming_controller.go:291] Starting NamingConditionController
I0917 18:21:05.513630       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 18:21:05.513710       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0917 18:21:05.514075       1 establishing_controller.go:76] Starting EstablishingController
I0917 18:21:05.514163       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0917 18:21:05.514192       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0917 18:21:05.514223       1 crd_finalizer.go:266] Starting CRDFinalizer
I0917 18:21:05.610534       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0917 18:21:05.611176       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0917 18:21:05.611196       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0917 18:21:05.611927       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0917 18:21:05.612545       1 shared_informer.go:318] Caches are synced for configmaps
I0917 18:21:05.612659       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0917 18:21:05.613256       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
E0917 18:21:05.613695       1 controller.go:155] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0917 18:21:05.614315       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0917 18:21:05.614508       1 aggregator.go:152] initial CRD sync complete...
I0917 18:21:05.614677       1 autoregister_controller.go:141] Starting autoregister controller
I0917 18:21:05.614741       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0917 18:21:05.614852       1 cache.go:39] Caches are synced for autoregister controller
I0917 18:21:05.704024       1 shared_informer.go:318] Caches are synced for node_authorizer
I0917 18:21:06.130129       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0917 18:21:06.515325       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0917 18:21:08.329930       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0917 18:21:08.418782       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0917 18:21:08.609458       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0917 18:21:08.711403       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0917 18:21:08.719522       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E0917 18:21:15.891464       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 32c286f5-4cfe-4ebd-a337-81573a7d68a2, UID in object meta: "
I0917 18:21:18.441772       1 controller.go:624] quota admission added evaluator for: endpoints
I0917 18:21:18.626512       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0917 18:21:18.626513       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0917 18:24:20.504531       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0917 18:26:19.928785       1 alloc.go:330] "allocated clusterIPs" service="default/myserverapp-service" clusterIPs=map[IPv4:10.101.124.212]

* 
* ==> kube-controller-manager [60fd3eb5059d] <==
* I0917 18:21:18.354039       1 certificate_controller.go:112] Starting certificate controller "csrsigning-kube-apiserver-client"
I0917 18:21:18.354075       1 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0917 18:21:18.354144       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0917 18:21:18.354626       1 controllermanager.go:638] "Started controller" controller="csrsigning"
I0917 18:21:18.354880       1 certificate_controller.go:112] Starting certificate controller "csrsigning-legacy-unknown"
I0917 18:21:18.354907       1 shared_informer.go:311] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0917 18:21:18.354940       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0917 18:21:18.360013       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0917 18:21:18.405235       1 shared_informer.go:318] Caches are synced for PVC protection
I0917 18:21:18.409117       1 shared_informer.go:318] Caches are synced for TTL after finished
I0917 18:21:18.409835       1 shared_informer.go:318] Caches are synced for crt configmap
I0917 18:21:18.412842       1 shared_informer.go:318] Caches are synced for expand
I0917 18:21:18.413594       1 shared_informer.go:318] Caches are synced for ephemeral
I0917 18:21:18.413638       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0917 18:21:18.421110       1 shared_informer.go:318] Caches are synced for ReplicationController
I0917 18:21:18.421393       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0917 18:21:18.421888       1 shared_informer.go:318] Caches are synced for cronjob
I0917 18:21:18.422651       1 shared_informer.go:318] Caches are synced for stateful set
I0917 18:21:18.424470       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0917 18:21:18.429334       1 shared_informer.go:318] Caches are synced for PV protection
I0917 18:21:18.432574       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0917 18:21:18.432608       1 shared_informer.go:318] Caches are synced for HPA
I0917 18:21:18.432622       1 shared_informer.go:318] Caches are synced for endpoint
I0917 18:21:18.432638       1 shared_informer.go:318] Caches are synced for job
I0917 18:21:18.432654       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0917 18:21:18.445961       1 shared_informer.go:318] Caches are synced for namespace
I0917 18:21:18.449308       1 shared_informer.go:318] Caches are synced for service account
I0917 18:21:18.452832       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0917 18:21:18.454046       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0917 18:21:18.454101       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0917 18:21:18.455279       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0917 18:21:18.507750       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0917 18:21:18.518434       1 shared_informer.go:318] Caches are synced for deployment
I0917 18:21:18.560721       1 shared_informer.go:318] Caches are synced for resource quota
I0917 18:21:18.562120       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0917 18:21:18.610136       1 shared_informer.go:318] Caches are synced for disruption
I0917 18:21:18.610515       1 shared_informer.go:318] Caches are synced for resource quota
I0917 18:21:18.617632       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0917 18:21:18.619143       1 shared_informer.go:318] Caches are synced for GC
I0917 18:21:18.619212       1 shared_informer.go:318] Caches are synced for daemon sets
I0917 18:21:18.622897       1 shared_informer.go:318] Caches are synced for node
I0917 18:21:18.622983       1 range_allocator.go:174] "Sending events to api server"
I0917 18:21:18.623015       1 range_allocator.go:178] "Starting range CIDR allocator"
I0917 18:21:18.623019       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0917 18:21:18.623024       1 shared_informer.go:318] Caches are synced for cidrallocator
I0917 18:21:18.630546       1 shared_informer.go:318] Caches are synced for taint
I0917 18:21:18.630712       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0917 18:21:18.630874       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0917 18:21:18.631279       1 taint_manager.go:211] "Sending events to api server"
I0917 18:21:18.631430       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0917 18:21:18.631518       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0917 18:21:18.634271       1 shared_informer.go:318] Caches are synced for TTL
I0917 18:21:18.635181       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0917 18:21:18.637519       1 shared_informer.go:318] Caches are synced for persistent volume
I0917 18:21:18.645163       1 shared_informer.go:318] Caches are synced for attach detach
I0917 18:21:19.014787       1 shared_informer.go:318] Caches are synced for garbage collector
I0917 18:21:19.027849       1 shared_informer.go:318] Caches are synced for garbage collector
I0917 18:21:19.027907       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0917 18:24:20.508232       1 event.go:307] "Event occurred" object="default/myserverapp-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set myserverapp-deployment-7d8767bf8b to 1"
I0917 18:24:20.527070       1 event.go:307] "Event occurred" object="default/myserverapp-deployment-7d8767bf8b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: myserverapp-deployment-7d8767bf8b-wc4rx"

* 
* ==> kube-controller-manager [a9e8ac9d179c] <==
* I0913 14:40:40.960160       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0913 14:40:40.961100       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0913 14:40:40.962182       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0913 14:40:40.963451       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0913 14:40:40.963518       1 shared_informer.go:318] Caches are synced for PV protection
I0913 14:40:40.963546       1 shared_informer.go:318] Caches are synced for TTL after finished
I0913 14:40:40.966146       1 shared_informer.go:318] Caches are synced for expand
I0913 14:40:40.966257       1 shared_informer.go:318] Caches are synced for ephemeral
I0913 14:40:40.968737       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0913 14:40:40.968793       1 shared_informer.go:318] Caches are synced for crt configmap
I0913 14:40:40.972547       1 shared_informer.go:318] Caches are synced for endpoint
I0913 14:40:40.972951       1 shared_informer.go:318] Caches are synced for service account
I0913 14:40:40.975544       1 shared_informer.go:318] Caches are synced for ReplicationController
I0913 14:40:40.976805       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0913 14:40:40.977216       1 shared_informer.go:318] Caches are synced for job
I0913 14:40:40.980154       1 shared_informer.go:318] Caches are synced for disruption
I0913 14:40:40.983493       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0913 14:40:40.990610       1 shared_informer.go:318] Caches are synced for namespace
I0913 14:40:40.991861       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0913 14:40:40.993083       1 shared_informer.go:318] Caches are synced for deployment
I0913 14:40:40.995906       1 shared_informer.go:318] Caches are synced for stateful set
I0913 14:40:40.999325       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0913 14:40:41.059456       1 shared_informer.go:318] Caches are synced for cronjob
I0913 14:40:41.088845       1 shared_informer.go:318] Caches are synced for HPA
I0913 14:40:41.089906       1 shared_informer.go:318] Caches are synced for resource quota
I0913 14:40:41.107265       1 shared_informer.go:318] Caches are synced for resource quota
I0913 14:40:41.107907       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0913 14:40:41.165978       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0913 14:40:41.169813       1 shared_informer.go:318] Caches are synced for node
I0913 14:40:41.169890       1 range_allocator.go:174] "Sending events to api server"
I0913 14:40:41.169919       1 range_allocator.go:178] "Starting range CIDR allocator"
I0913 14:40:41.169923       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0913 14:40:41.169927       1 shared_informer.go:318] Caches are synced for cidrallocator
I0913 14:40:41.171006       1 shared_informer.go:318] Caches are synced for GC
I0913 14:40:41.173914       1 shared_informer.go:318] Caches are synced for taint
I0913 14:40:41.174068       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0913 14:40:41.174172       1 taint_manager.go:211] "Sending events to api server"
I0913 14:40:41.173999       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0913 14:40:41.174468       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0913 14:40:41.174575       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0913 14:40:41.174612       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0913 14:40:41.193683       1 shared_informer.go:318] Caches are synced for attach detach
I0913 14:40:41.196089       1 shared_informer.go:318] Caches are synced for daemon sets
I0913 14:40:41.197498       1 shared_informer.go:318] Caches are synced for TTL
I0913 14:40:41.203986       1 shared_informer.go:318] Caches are synced for persistent volume
I0913 14:40:41.515762       1 shared_informer.go:318] Caches are synced for garbage collector
I0913 14:40:41.576492       1 shared_informer.go:318] Caches are synced for garbage collector
I0913 14:40:41.576556       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0913 15:19:37.694357       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I0913 15:19:37.705469       1 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:37.790860       1 event.go:307] "Event occurred" object="kube-system/kube-proxy-hjkp6" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:37.809859       1 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:37.890827       1 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:37.897220       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-p852n" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:37.908143       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d-2s955" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:37.986261       1 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:38.002779       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-snftb" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:38.088005       1 event.go:307] "Event occurred" object="default/hello-minikube-59d4768566-nwvfr" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0913 15:19:38.095116       1 node_lifecycle_controller.go:1027] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0913 15:19:38.095140       1 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"

* 
* ==> kube-proxy [334bbc0583d2] <==
* I0917 18:21:09.134616       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0917 18:21:09.134817       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0917 18:21:09.134987       1 server_others.go:554] "Using iptables proxy"
I0917 18:21:09.329194       1 server_others.go:192] "Using iptables Proxier"
I0917 18:21:09.329263       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0917 18:21:09.329273       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0917 18:21:09.329298       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0917 18:21:09.330040       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0917 18:21:09.335163       1 server.go:658] "Version info" version="v1.27.4"
I0917 18:21:09.335206       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 18:21:09.404105       1 config.go:188] "Starting service config controller"
I0917 18:21:09.404263       1 config.go:97] "Starting endpoint slice config controller"
I0917 18:21:09.410828       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0917 18:21:09.404460       1 config.go:315] "Starting node config controller"
I0917 18:21:09.410923       1 shared_informer.go:311] Waiting for caches to sync for node config
I0917 18:21:09.405415       1 shared_informer.go:311] Waiting for caches to sync for service config
I0917 18:21:09.512042       1 shared_informer.go:318] Caches are synced for service config
I0917 18:21:09.512136       1 shared_informer.go:318] Caches are synced for node config
I0917 18:21:09.512219       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [d2c6077374dc] <==
* E0913 14:40:25.466008       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I0913 14:40:28.561823       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0913 14:40:28.561907       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0913 14:40:28.561940       1 server_others.go:554] "Using iptables proxy"
I0913 14:40:28.662263       1 server_others.go:192] "Using iptables Proxier"
I0913 14:40:28.662381       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0913 14:40:28.662391       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0913 14:40:28.662407       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0913 14:40:28.662501       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0913 14:40:28.663452       1 server.go:658] "Version info" version="v1.27.4"
I0913 14:40:28.663842       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0913 14:40:28.664837       1 config.go:188] "Starting service config controller"
I0913 14:40:28.664901       1 shared_informer.go:311] Waiting for caches to sync for service config
I0913 14:40:28.664935       1 config.go:97] "Starting endpoint slice config controller"
I0913 14:40:28.664940       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0913 14:40:28.665436       1 config.go:315] "Starting node config controller"
I0913 14:40:28.665473       1 shared_informer.go:311] Waiting for caches to sync for node config
I0913 14:40:28.766036       1 shared_informer.go:318] Caches are synced for node config
I0913 14:40:28.766090       1 shared_informer.go:318] Caches are synced for service config
I0913 14:40:28.766102       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [028c841a8af8] <==
* I0913 14:40:27.059729       1 serving.go:348] Generated self-signed cert in-memory
W0913 14:40:28.376280       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0913 14:40:28.376584       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0913 14:40:28.376706       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0913 14:40:28.376870       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0913 14:40:28.463881       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0913 14:40:28.463939       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0913 14:40:28.466023       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0913 14:40:28.466506       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0913 14:40:28.466556       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0913 14:40:28.466578       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0913 14:40:28.567525       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [f2487cd3e7aa] <==
* I0917 18:21:03.006089       1 serving.go:348] Generated self-signed cert in-memory
W0917 18:21:05.610943       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0917 18:21:05.611022       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0917 18:21:05.611040       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0917 18:21:05.611047       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0917 18:21:05.628414       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0917 18:21:05.628460       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 18:21:05.630239       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 18:21:05.630350       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 18:21:05.631145       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0917 18:21:05.631290       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0917 18:21:05.730968       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Sep 17 18:27:09 minikube kubelet[1481]: E0917 18:27:09.225130    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:27:23 minikube kubelet[1481]: E0917 18:27:23.940694    1481 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="myserverapp:v1"
Sep 17 18:27:23 minikube kubelet[1481]: E0917 18:27:23.940781    1481 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="myserverapp:v1"
Sep 17 18:27:23 minikube kubelet[1481]: E0917 18:27:23.940902    1481 kuberuntime_manager.go:1212] container &Container{Name:myserverapp-container,Image:myserverapp:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kgq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod myserverapp-deployment-7d8767bf8b-wc4rx_default(14fa783a-4e16-41ef-82e5-ceb56907c826): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Sep 17 18:27:23 minikube kubelet[1481]: E0917 18:27:23.940943    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:27:37 minikube kubelet[1481]: E0917 18:27:37.225401    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:27:49 minikube kubelet[1481]: E0917 18:27:49.226527    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:28:01 minikube kubelet[1481]: E0917 18:28:01.225024    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:28:13 minikube kubelet[1481]: E0917 18:28:13.226096    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:28:28 minikube kubelet[1481]: E0917 18:28:28.225044    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:28:40 minikube kubelet[1481]: E0917 18:28:40.226296    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:28:54 minikube kubelet[1481]: E0917 18:28:54.226443    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:29:06 minikube kubelet[1481]: E0917 18:29:06.225523    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:29:17 minikube kubelet[1481]: E0917 18:29:17.225566    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:29:32 minikube kubelet[1481]: E0917 18:29:32.226254    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:29:45 minikube kubelet[1481]: E0917 18:29:45.226571    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:29:56 minikube kubelet[1481]: E0917 18:29:56.227051    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:30:10 minikube kubelet[1481]: E0917 18:30:10.593243    1481 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="myserverapp:v1"
Sep 17 18:30:10 minikube kubelet[1481]: E0917 18:30:10.593326    1481 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="myserverapp:v1"
Sep 17 18:30:10 minikube kubelet[1481]: E0917 18:30:10.593447    1481 kuberuntime_manager.go:1212] container &Container{Name:myserverapp-container,Image:myserverapp:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kgq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod myserverapp-deployment-7d8767bf8b-wc4rx_default(14fa783a-4e16-41ef-82e5-ceb56907c826): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Sep 17 18:30:10 minikube kubelet[1481]: E0917 18:30:10.593491    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:30:25 minikube kubelet[1481]: E0917 18:30:25.227592    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:30:40 minikube kubelet[1481]: E0917 18:30:40.226931    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:30:51 minikube kubelet[1481]: E0917 18:30:51.226937    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:30:59 minikube kubelet[1481]: W0917 18:30:59.331195    1481 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Sep 17 18:31:05 minikube kubelet[1481]: E0917 18:31:05.227863    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:31:17 minikube kubelet[1481]: E0917 18:31:17.227029    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:31:32 minikube kubelet[1481]: E0917 18:31:32.227590    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:31:43 minikube kubelet[1481]: E0917 18:31:43.227233    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:31:54 minikube kubelet[1481]: E0917 18:31:54.228293    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:32:06 minikube kubelet[1481]: E0917 18:32:06.227559    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:32:17 minikube kubelet[1481]: E0917 18:32:17.227813    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:32:32 minikube kubelet[1481]: E0917 18:32:32.228005    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:32:44 minikube kubelet[1481]: E0917 18:32:44.228147    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:32:57 minikube kubelet[1481]: E0917 18:32:57.228668    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:33:11 minikube kubelet[1481]: E0917 18:33:11.229817    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:33:25 minikube kubelet[1481]: E0917 18:33:25.229270    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:33:36 minikube kubelet[1481]: E0917 18:33:36.229295    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:33:47 minikube kubelet[1481]: E0917 18:33:47.229293    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:34:00 minikube kubelet[1481]: E0917 18:34:00.229635    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:34:11 minikube kubelet[1481]: E0917 18:34:11.230260    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:34:25 minikube kubelet[1481]: E0917 18:34:25.229312    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:34:37 minikube kubelet[1481]: E0917 18:34:37.230156    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:34:50 minikube kubelet[1481]: E0917 18:34:50.230154    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:35:05 minikube kubelet[1481]: E0917 18:35:05.230188    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:35:20 minikube kubelet[1481]: E0917 18:35:20.474169    1481 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="myserverapp:v1"
Sep 17 18:35:20 minikube kubelet[1481]: E0917 18:35:20.474255    1481 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="myserverapp:v1"
Sep 17 18:35:20 minikube kubelet[1481]: E0917 18:35:20.474371    1481 kuberuntime_manager.go:1212] container &Container{Name:myserverapp-container,Image:myserverapp:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5kgq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod myserverapp-deployment-7d8767bf8b-wc4rx_default(14fa783a-4e16-41ef-82e5-ceb56907c826): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Sep 17 18:35:20 minikube kubelet[1481]: E0917 18:35:20.474414    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for myserverapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:35:34 minikube kubelet[1481]: E0917 18:35:34.230874    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:35:48 minikube kubelet[1481]: E0917 18:35:48.229389    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:35:59 minikube kubelet[1481]: W0917 18:35:59.336973    1481 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Sep 17 18:36:03 minikube kubelet[1481]: E0917 18:36:03.234615    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:36:16 minikube kubelet[1481]: E0917 18:36:16.234367    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:36:27 minikube kubelet[1481]: E0917 18:36:27.235027    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:36:40 minikube kubelet[1481]: E0917 18:36:40.235400    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:36:55 minikube kubelet[1481]: E0917 18:36:55.236119    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:37:08 minikube kubelet[1481]: E0917 18:37:08.236167    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:37:23 minikube kubelet[1481]: E0917 18:37:23.235883    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826
Sep 17 18:37:35 minikube kubelet[1481]: E0917 18:37:35.236928    1481 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myserverapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"myserverapp:v1\\\"\"" pod="default/myserverapp-deployment-7d8767bf8b-wc4rx" podUID=14fa783a-4e16-41ef-82e5-ceb56907c826

* 
* ==> kubernetes-dashboard [cc7a564bafd8] <==
* 2023/09/17 18:21:54 Starting overwatch
2023/09/17 18:21:54 Using namespace: kubernetes-dashboard
2023/09/17 18:21:54 Using in-cluster config to connect to apiserver
2023/09/17 18:21:54 Using secret token for csrf signing
2023/09/17 18:21:54 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/09/17 18:21:54 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/09/17 18:21:54 Successful initial request to the apiserver, version: v1.27.4
2023/09/17 18:21:54 Generating JWE encryption key
2023/09/17 18:21:54 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/09/17 18:21:54 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/09/17 18:21:54 Initializing JWE encryption key from synchronized object
2023/09/17 18:21:54 Creating in-cluster Sidecar client
2023/09/17 18:21:54 Successful request to sidecar
2023/09/17 18:21:54 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [eedf7ebbf07f] <==
* 2023/09/17 18:21:09 Using namespace: kubernetes-dashboard
2023/09/17 18:21:09 Using in-cluster config to connect to apiserver
2023/09/17 18:21:09 Using secret token for csrf signing
2023/09/17 18:21:09 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/09/17 18:21:09 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00069fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000244080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [7948ab3aef7b] <==
* I0917 18:22:05.324913       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0917 18:22:05.338554       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0917 18:22:05.339648       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0917 18:22:22.753754       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0917 18:22:22.755164       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_59269910-6b33-4e31-b762-9a32abbdfd68!
I0917 18:22:22.755940       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"32d5d932-e58c-4419-be5c-fb61cc4e9fe9", APIVersion:"v1", ResourceVersion:"3836", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_59269910-6b33-4e31-b762-9a32abbdfd68 became leader
I0917 18:22:22.858599       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_59269910-6b33-4e31-b762-9a32abbdfd68!

* 
* ==> storage-provisioner [d22d55502196] <==
* I0917 18:21:10.109729       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0917 18:21:40.120134       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

